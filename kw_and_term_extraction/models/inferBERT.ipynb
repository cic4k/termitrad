{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fac60da-dbb8-4f4a-9338-05d3390da392",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148dc3c6-6d95-4a49-b35e-983fb83941b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8452d41c-b7f3-45fa-81b6-22a3306679bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eat_vf_df = pd.read_excel('./sample_data/ingénieriesEAT-VF-2021030.xlsx').rename(columns={'Gestion et traitement des déjections animales en Italie = ':'fr_title_s'})\n",
    "set_vf_df = pd.read_excel('./sample_data/SET-VF20210211.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4bc502-1d83-4f39-900b-9e2977aa8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 696 entries, 0 to 695\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   docType_s       696 non-null    object\n",
      " 1   halId_s         696 non-null    object\n",
      " 2   fr_title_s      696 non-null    object\n",
      " 3   en_title_s      696 non-null    object\n",
      " 4   fr_abstract_s   696 non-null    object\n",
      " 5   en_abstract_s4  696 non-null    object\n",
      " 6   fr_keyword_s    696 non-null    object\n",
      " 7   en_keyword_s    696 non-null    object\n",
      " 8   producedDate_s  696 non-null    object\n",
      " 9   volume_s        9 non-null      object\n",
      " 10  issue_s         688 non-null    object\n",
      " 11  uri_s           696 non-null    object\n",
      "dtypes: object(12)\n",
      "memory usage: 65.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docType_s</th>\n",
       "      <th>halId_s</th>\n",
       "      <th>fr_title_s</th>\n",
       "      <th>en_title_s</th>\n",
       "      <th>fr_abstract_s</th>\n",
       "      <th>en_abstract_s4</th>\n",
       "      <th>fr_keyword_s</th>\n",
       "      <th>en_keyword_s</th>\n",
       "      <th>producedDate_s</th>\n",
       "      <th>volume_s</th>\n",
       "      <th>issue_s</th>\n",
       "      <th>uri_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ART</td>\n",
       "      <td>hal-02582981</td>\n",
       "      <td>Aménagement du Rhône et débit réservé</td>\n",
       "      <td>Development of the Rhône and reserved flow</td>\n",
       "      <td>Les opérations d'augmentation de débits réserv...</td>\n",
       "      <td>Operations to increase the reserve flows and t...</td>\n",
       "      <td>Aménagement de cours d'eau, débit réservé, éne...</td>\n",
       "      <td>River management, instream flow, hydroelectric...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38 supplément</td>\n",
       "      <td>https://hal.inrae.fr/hal-02582981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ART</td>\n",
       "      <td>hal-02580960</td>\n",
       "      <td>Aménité : qualité des relations sociales, qual...</td>\n",
       "      <td>Amenity: quality in the social relationships, ...</td>\n",
       "      <td>Dans un contexte où le mot \"aménités\" circule ...</td>\n",
       "      <td>In a context where the word 'amenities' is use...</td>\n",
       "      <td>Aménité, sciences humaines et sociales, dévelo...</td>\n",
       "      <td>Amenity, human and social sciences, rural deve...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spécial Aménités rurales : une nouvelle lectur...</td>\n",
       "      <td>https://hal.inrae.fr/hal-02580960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docType_s       halId_s                                         fr_title_s  \\\n",
       "0       ART  hal-02582981              Aménagement du Rhône et débit réservé   \n",
       "1       ART  hal-02580960  Aménité : qualité des relations sociales, qual...   \n",
       "\n",
       "                                          en_title_s  \\\n",
       "0         Development of the Rhône and reserved flow   \n",
       "1  Amenity: quality in the social relationships, ...   \n",
       "\n",
       "                                       fr_abstract_s  \\\n",
       "0  Les opérations d'augmentation de débits réserv...   \n",
       "1  Dans un contexte où le mot \"aménités\" circule ...   \n",
       "\n",
       "                                      en_abstract_s4  \\\n",
       "0  Operations to increase the reserve flows and t...   \n",
       "1  In a context where the word 'amenities' is use...   \n",
       "\n",
       "                                        fr_keyword_s  \\\n",
       "0  Aménagement de cours d'eau, débit réservé, éne...   \n",
       "1  Aménité, sciences humaines et sociales, dévelo...   \n",
       "\n",
       "                                        en_keyword_s producedDate_s volume_s  \\\n",
       "0  River management, instream flow, hydroelectric...         2004.0      NaN   \n",
       "1  Amenity, human and social sciences, rural deve...         2002.0      NaN   \n",
       "\n",
       "                                             issue_s  \\\n",
       "0                                      38 supplément   \n",
       "1  spécial Aménités rurales : une nouvelle lectur...   \n",
       "\n",
       "                               uri_s  \n",
       "0  https://hal.inrae.fr/hal-02582981  \n",
       "1  https://hal.inrae.fr/hal-02580960  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat_vf_df.info()\n",
    "eat_vf_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80173f04-318a-4e00-a01f-a28c4134ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 287 entries, 0 to 286\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Titre FR      287 non-null    object\n",
      " 1   Titre GB      287 non-null    object\n",
      " 2   RésuméFR      287 non-null    object\n",
      " 3   Résumé GB     287 non-null    object\n",
      " 4   Mots-clés FR  287 non-null    object\n",
      " 5   Mots clés GB  287 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 13.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre FR</th>\n",
       "      <th>Titre GB</th>\n",
       "      <th>RésuméFR</th>\n",
       "      <th>Résumé GB</th>\n",
       "      <th>Mots-clés FR</th>\n",
       "      <th>Mots clés GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Une approche innovante de modélisation du risq...</td>\n",
       "      <td>An innovative approach to modelling forest fir...</td>\n",
       "      <td>Une méthode de cartographie des interfaces hab...</td>\n",
       "      <td>A method to characterize and to map wildland-u...</td>\n",
       "      <td>Interface habitat-forêt, risque d’incendie, dé...</td>\n",
       "      <td>Wildland-urban interface, fire risk, fire star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mise en œuvre de deux mesures agrienvironnemen...</td>\n",
       "      <td>Implementation of two result-based agrienviron...</td>\n",
       "      <td>Afin de répondre aux exigences de la société e...</td>\n",
       "      <td>In order to meet society's demands for a more ...</td>\n",
       "      <td>Mesure agroenvironnementale, prairie fleurie, ...</td>\n",
       "      <td>Agrienvironmental measure, flowering meadow, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Titre FR  \\\n",
       "0  Une approche innovante de modélisation du risq...   \n",
       "1  Mise en œuvre de deux mesures agrienvironnemen...   \n",
       "\n",
       "                                            Titre GB  \\\n",
       "0  An innovative approach to modelling forest fir...   \n",
       "1  Implementation of two result-based agrienviron...   \n",
       "\n",
       "                                            RésuméFR  \\\n",
       "0  Une méthode de cartographie des interfaces hab...   \n",
       "1  Afin de répondre aux exigences de la société e...   \n",
       "\n",
       "                                           Résumé GB  \\\n",
       "0  A method to characterize and to map wildland-u...   \n",
       "1  In order to meet society's demands for a more ...   \n",
       "\n",
       "                                        Mots-clés FR  \\\n",
       "0  Interface habitat-forêt, risque d’incendie, dé...   \n",
       "1  Mesure agroenvironnementale, prairie fleurie, ...   \n",
       "\n",
       "                                        Mots clés GB  \n",
       "0  Wildland-urban interface, fire risk, fire star...  \n",
       "1  Agrienvironmental measure, flowering meadow, p...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_vf_df.info()\n",
    "set_vf_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e80a088-0a9f-4baf-af54-a224dd338424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_standard(df, col):\n",
    "    gold_standard = []\n",
    "    for x in df[col]:\n",
    "        gold_standard.extend(x.split(',')) \n",
    "    gold_standard = set([x.strip().lower() for x in gold_standard])\n",
    "    return gold_standard\n",
    "\n",
    "def create_gold_standard_(df, col):\n",
    "    gold_standard = []\n",
    "    for x in df[col]:\n",
    "        gold_standard.extend(x) \n",
    "    gold_standard = set([x.strip().lower() for x in gold_standard])\n",
    "    return gold_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082f04f3-f78f-42ff-a5b9-7e63c90fa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_eat_en = create_gold_standard(eat_vf_df, 'en_keyword_s')\n",
    "gold_standard_eat_fr = create_gold_standard(eat_vf_df, 'fr_keyword_s')\n",
    "gold_standard_set_en = create_gold_standard(set_vf_df, 'Mots clés GB')\n",
    "gold_standard_set_fr = create_gold_standard(set_vf_df, 'Mots-clés FR')\n",
    "gold_standard_eat_en.remove('')\n",
    "gold_standard_eat_fr.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d770e059-97e9-4862-a9ac-7228b04584fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_kw(df, keywords, corpus, groundtruth):\n",
    "    df[groundtruth] = pd.Series(dtype='object')\n",
    "    for i in range(len(df)):\n",
    "        df[groundtruth].iloc[i] = [x for x in df[keywords].iloc[i].split(',') if x.strip().lower() in df[corpus].iloc[i].lower()]\n",
    "    return df\n",
    "\n",
    "eat_vf_df['en_title_abstract'] = eat_vf_df['en_title_s'] + eat_vf_df['en_abstract_s4']\n",
    "eat_vf_df['fr_title_abstract'] = eat_vf_df['fr_title_s'] + eat_vf_df['fr_abstract_s']\n",
    "set_vf_df['en_title_abstract'] = set_vf_df['Titre GB'] + set_vf_df['Résumé GB']\n",
    "set_vf_df['fr_title_abstract'] = set_vf_df['Titre FR'] + set_vf_df['RésuméFR']\n",
    "\n",
    "eat_vf_df = get_true_kw(eat_vf_df, 'en_keyword_s', 'en_title_abstract', 'en_kw_known')\n",
    "eat_vf_df = get_true_kw(eat_vf_df, 'fr_keyword_s', 'fr_title_abstract', 'fr_kw_known')\n",
    "set_vf_df = get_true_kw(set_vf_df,  'Mots clés GB', 'en_title_abstract', 'en_kw_known')\n",
    "set_vf_df = get_true_kw(set_vf_df, 'Mots-clés FR', 'fr_title_abstract', 'fr_kw_known')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489d48c0-00eb-4a53-aa13-6c7baef79a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_eat_en_ = create_gold_standard_(eat_vf_df, 'en_kw_known')\n",
    "gold_standard_eat_fr_ = create_gold_standard_(eat_vf_df, 'fr_kw_known')\n",
    "gold_standard_set_en_ = create_gold_standard_(set_vf_df, 'en_kw_known')\n",
    "gold_standard_set_fr_ = create_gold_standard_(set_vf_df, 'fr_kw_known')\n",
    "gold_standard_eat_en_.remove('')\n",
    "gold_standard_eat_fr_.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99937df1-b284-4766-aa27-b3205a5773bb",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacc46cb-7439-46ff-9acf-af7f262f8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pickle\n",
    "# import argparse\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification   \n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# from utils import *\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     # Remove ignored index (special tokens)\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "\n",
    "#     extracted_terms = extract_terms(true_predictions, val)\n",
    "#     extracted_terms = set([item.lower() for item in extracted_terms])\n",
    "#     gold_set=gold_validation     \n",
    "\n",
    "\n",
    "#     true_pos=extracted_terms.intersection(gold_set)\n",
    "    \n",
    "#     recall=len(true_pos)/len(gold_set) if len(gold_set) != 0 else 0\n",
    "#     precision=len(true_pos)/len(extracted_terms) if len(extracted_terms) != 0 else 0\n",
    "#     f1 = 2*(precision*recall)/(precision+recall) if (precision+recall) != 0 else 0\n",
    "\n",
    "#     return {\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1\": f1,\n",
    "#     }\n",
    "\n",
    "# label_list=[\"O\", \"B\", \"I\"]\n",
    "# label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./pretrained_models/model_en_roberta/')\n",
    "# model = AutoModelForTokenClassification.from_pretrained('./pretrained_models/model_en_roberta/', num_labels=len(label_list))\n",
    "\n",
    "# test_args = TrainingArguments(\n",
    "#     output_dir= './',          # output directory\n",
    "#     do_train = False,\n",
    "#     do_predict = True,\n",
    "#     per_device_eval_batch_size=32,   # batch size for evaluation  \n",
    "#     dataloader_drop_last = False   \n",
    "# )\n",
    "\n",
    "# # init trainer\n",
    "# trainer = Trainer(\n",
    "#               model = model, \n",
    "#               args = test_args, \n",
    "#               compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72bf1584-c5d6-4b32-9762-061a08d30db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in eat_vf_df['en_title_abstract']]\n",
    "# tags =  [['O']*len(x.split()) for x in eat_vf_df['en_title_abstract']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_en))\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_en_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22925ffd-a3ff-46b7-b7bd-0b272bda1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in set_vf_df['en_title_abstract']]\n",
    "# tags =  [['O']*len(x.split()) for x in set_vf_df['en_title_abstract']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_en))\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_en_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70ee596-f273-481e-9e97-869c66bc1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in eat_vf_df['en_abstract_s4']]\n",
    "# tags =  [['O']*len(x.split()) for x in eat_vf_df['en_abstract_s4']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8562757-5df4-4376-a22d-c2ab9e6ef3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in eat_vf_df['en_title_s']]\n",
    "# tags =  [['O']*len(x.split()) for x in eat_vf_df['en_title_s']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6d2cc7-3e31-409e-84b6-1773ec57e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in set_vf_df['Titre GB']]\n",
    "# tags =  [['O']*len(x.split()) for x in set_vf_df['Titre GB']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4c203b-38b1-4f39-a779-b676e8973a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in set_vf_df['Résumé GB']]\n",
    "# tags =  [['O']*len(x.split()) for x in set_vf_df['Résumé GB']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae11552-619c-4636-9dbc-46fbb59fded6",
   "metadata": {},
   "source": [
    "# FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "065ab742-e53d-4cfe-928d-6715257ba302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tranthh/anaconda3/envs/term_torch18/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification   \n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    extracted_terms = extract_terms(true_predictions, val)\n",
    "    extracted_terms = set([item.lower() for item in extracted_terms])\n",
    "    gold_set=gold_validation     \n",
    "\n",
    "\n",
    "    true_pos=extracted_terms.intersection(gold_set)\n",
    "    \n",
    "    recall=len(true_pos)/len(gold_set) if len(gold_set) != 0 else 0\n",
    "    precision=len(true_pos)/len(extracted_terms) if len(extracted_terms) != 0 else 0\n",
    "    f1 = 2*(precision*recall)/(precision+recall) if (precision+recall) != 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "label_list=[\"O\", \"B\", \"I\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "tokenizer = AutoTokenizer.from_pretrained('./pretrained_models/model_fr_camembert/')\n",
    "model = AutoModelForTokenClassification.from_pretrained('./pretrained_models/model_fr_camembert/', num_labels=len(label_list))\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir= './',          # output directory\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation  \n",
    "    dataloader_drop_last = False   \n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "              model = model, \n",
    "              args = test_args, \n",
    "              compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a9d12a-d2ec-41b1-b7ee-9506c1d3c3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 696\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2791\n",
      "Gold 1339\n",
      "Intersection 233\n",
      "Precision: 8.35\n",
      "Recall: 17.4\n",
      "F1: 11.28\n",
      "2791 | 1339 | 233 | 8.35 & 17.4 & 11.28\n",
      "Extracted 2791\n",
      "Gold 565\n",
      "Intersection 202\n",
      "Precision: 7.24\n",
      "Recall: 35.75\n",
      "F1: 12.04\n",
      "2791 | 565 | 202 | 7.24 & 35.75 & 12.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2791, 565, 202, 7.24, 35.75, 12.04)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [x.split() for x in eat_vf_df['fr_title_abstract']]\n",
    "tags =  [['O']*len(x.split()) for x in eat_vf_df['fr_title_abstract']]\n",
    "val = texts\n",
    "gold_validation = 'O'\n",
    "input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "predictions, labels, metrics = trainer.predict(dataset)\n",
    "predictions1 = np.argmax(predictions, axis=2)\n",
    "true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "                    for prediction1, label in zip(predictions1, labels)]\n",
    "extracted_terms = extract_terms(true_predictions, texts)\n",
    "computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_fr))\n",
    "computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_fr_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8e41e2-bbb9-4343-82e9-fe9df636509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 287\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1540\n",
      "Gold 638\n",
      "Intersection 112\n",
      "Precision: 7.27\n",
      "Recall: 17.55\n",
      "F1: 10.28\n",
      "1540 | 638 | 112 | 7.27 & 17.55 & 10.28\n",
      "Extracted 1540\n",
      "Gold 277\n",
      "Intersection 106\n",
      "Precision: 6.88\n",
      "Recall: 38.27\n",
      "F1: 11.66\n",
      "1540 | 277 | 106 | 6.88 & 38.27 & 11.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1540, 277, 106, 6.88, 38.27, 11.66)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [x.split() for x in set_vf_df['fr_title_abstract']]\n",
    "tags =  [['O']*len(x.split()) for x in set_vf_df['fr_title_abstract']]\n",
    "val = texts\n",
    "gold_validation = 'O'\n",
    "input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "predictions, labels, metrics = trainer.predict(dataset)\n",
    "predictions1 = np.argmax(predictions, axis=2)\n",
    "true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "                    for prediction1, label in zip(predictions1, labels)]\n",
    "extracted_terms = extract_terms(true_predictions, texts)\n",
    "computeTermEvalMetrics(extracted_terms, set(gold_standard_set_fr))\n",
    "computeTermEvalMetrics(extracted_terms, set(gold_standard_set_fr_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04bc02d3-87c0-4532-8a9a-729a03477488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in eat_vf_df['fr_abstract_s']]\n",
    "# tags =  [['O']*len(x.split()) for x in eat_vf_df['fr_abstract_s']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94cfe76e-5da7-49b2-9dba-89b1c02e289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in eat_vf_df['fr_title_s']]\n",
    "# tags =  [['O']*len(x.split()) for x in eat_vf_df['fr_title_s']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_eat_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee3d7f2-f568-46cd-bffd-3eacfb95b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in set_vf_df['Titre FR']]\n",
    "# tags =  [['O']*len(x.split()) for x in set_vf_df['Titre FR']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf269a25-9929-41be-9a8e-613214903d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [x.split() for x in set_vf_df['RésuméFR']]\n",
    "# tags =  [['O']*len(x.split()) for x in set_vf_df['RésuméFR']]\n",
    "# val = texts\n",
    "# gold_validation = 'O'\n",
    "# input_and_labels = tokenize_and_align_labels(texts, tags, tokenizer, label_to_id)\n",
    "# dataset = OurDataset(input_and_labels, input_and_labels[\"labels\"])\n",
    "# predictions, labels, metrics = trainer.predict(dataset)\n",
    "# predictions1 = np.argmax(predictions, axis=2)\n",
    "# true_predictions = [[label_list[p] for (p, l) in zip(prediction1, label) if l != -100]\n",
    "#                     for prediction1, label in zip(predictions1, labels)]\n",
    "# extracted_terms = extract_terms(true_predictions, texts)\n",
    "# computeTermEvalMetrics(extracted_terms, set(gold_standard_set_fr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
